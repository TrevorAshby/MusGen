{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for training a music generation transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/libs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our file imports\n",
    "from music_transformer import *\n",
    "from transformer_training_helpers import *\n",
    "from song_classification import *\n",
    "from dataset import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# musicautobot\n",
    "from musicautobot.numpy_encode import *\n",
    "from musicautobot.config import *\n",
    "from musicautobot.music_transformer import *\n",
    "from musicautobot.utils.midifile import *\n",
    "from musicautobot.utils.file_processing import process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: DONT NEED\n",
    "#midi_path = '../src/mid_data_collections/mid_0_to_10000'\n",
    "# numpy_path = '../numpy_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1177"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: DONT NEED\n",
    "# numpy_files = get_files(numpy_path, extensions='.npy', recurse=True); len(numpy_files)\n",
    "#midi_files = get_files(midi_path, '.mid', recurse=True)\n",
    "#print(midi_files[0])\n",
    "#print(len(midi_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW DATASET CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,   1.,  79.,  ..., 139.,  63., 139.],\n",
      "         [  8., 138.,  79.,  ..., 139.,  68., 139.],\n",
      "         [ 51., 140.,   8.,  ..., 139.,  63., 139.],\n",
      "         ...,\n",
      "         [ 75., 138.,  70.,  ..., 138.,   8., 139.],\n",
      "         [ 80., 138.,  77.,  ..., 138.,  51., 140.],\n",
      "         [  8., 138.,  84.,  ..., 138.,  86., 138.]]])\n"
     ]
    }
   ],
   "source": [
    "myDs = MidiDataset('../numpy_path', 150, 'N/A', '.npy')\n",
    "dataloader = DataLoader(myDs, 1, shuffle=True)\n",
    "\n",
    "for _, item in enumerate(dataloader):\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END NEW DATASET CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: DONT NEED\n",
    "# data_path = './saved_data/'\n",
    "# data_save_name = 'music_item_data_0-10000.pkl'\n",
    "# def create_databunch(files, path):\n",
    "#     #save_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "#     vocab = MusicVocab.create()\n",
    "#     processors = [OpenNPFileProcessor(), MusicItemProcessor()]\n",
    "\n",
    "#     #data = MusicDataBunch.from_files(midi_files, path, processors=processors, encode_position=True)\n",
    "#     data = MusicDataBunch.from_files(files, path, processors=processors, encode_position=True)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trevi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\fastai\\core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MusicDataBunch;\n",
       "\n",
       "Train: LabelList (1060 items)\n",
       "x: MusicItemList\n",
       "\n",
       "MusicItem - (9818,)\n",
       "xxbos xxpad n54 d2 n50 d2 n38 d1 xxsep d1...,\n",
       "MusicItem - (6616,)\n",
       "xxbos xxpad n64 d2 n61 d2 n57 d2 n52 d4...,\n",
       "MusicItem - (9758,)\n",
       "xxbos xxpad xxsep d7 n54 d11 xxsep d1 n64 d11...,\n",
       "MusicItem - (1236,)\n",
       "xxbos xxpad n76 d32 n72 d32 n67 d32 n64 d32...,\n",
       "MusicItem - (7064,)\n",
       "xxbos xxpad n76 d2 n72 d2 n60 d29 n57 d28...\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: saved_data;\n",
       "\n",
       "Valid: LabelList (117 items)\n",
       "x: MusicItemList\n",
       "\n",
       "MusicItem - (11442,)\n",
       "xxbos xxpad xxsep d9 n55 d1 xxsep d1 n57 d1...,\n",
       "MusicItem - (15824,)\n",
       "xxbos xxpad xxsep d11 n89 d1 n88 d1 xxsep d1...,\n",
       "MusicItem - (2552,)\n",
       "xxbos xxpad n70 d10 n67 d10 n62 d10 n58 d10...,\n",
       "MusicItem - (5066,)\n",
       "xxbos xxpad n58 d1 n55 d1 n51 d1 n39 d4...,\n",
       "MusicItem - (5798,)\n",
       "xxbos xxpad n60 d128 n55 d32 n51 d32 n48 d12...\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: saved_data;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: DONT NEED\n",
    "# all_data = create_databunch(numpy_files, data_path); all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 150])\n",
      "torch.Size([65, 149])\n",
      "torch.Size([65, 149])\n",
      "torch.Size([65, 1, 150])\n",
      "torch.Size([65, 149, 149])\n"
     ]
    }
   ],
   "source": [
    "# the_train_data = data_gen(all_data.train_ds)\n",
    "# print(the_train_data[0].src.shape)\n",
    "# print(the_train_data[0].trg.shape)\n",
    "# print(the_train_data[0].trg_y.shape)\n",
    "# print(the_train_data[0].src_mask.shape)\n",
    "# print(the_train_data[0].trg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 150])\n",
      "beg\n",
      "torch.Size([36, 150])\n",
      "torch.Size([36, 149])\n",
      "torch.Size([36, 149])\n",
      "torch.Size([36, 1, 150])\n",
      "torch.Size([36, 149, 149])\n"
     ]
    }
   ],
   "source": [
    "# for _, batch in enumerate(dataloader):\n",
    "#     #out, w2 = model.forward(batch.src, batch.trg, \n",
    "#     #                    batch.src_mask, batch.trg_mask)\n",
    "#     print(batch.shape)\n",
    "#     print('beg')\n",
    "#     batch = batch.long()\n",
    "#     batch = batch.squeeze(0)\n",
    "#     PAD = 0\n",
    "#     trg = batch[:,:-1]\n",
    "#     trg_y = batch[:, 1:]\n",
    "#     src_mask = (batch != PAD).unsqueeze(-2)\n",
    "#     trg_mask = (trg != PAD).unsqueeze(-2)\n",
    "#     trg_mask = trg_mask & Variable(subsequent_mask(trg.size(-1)).type_as(trg_mask.data))\n",
    "#     print(batch.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(trg_y.shape)\n",
    "#     print(src_mask.shape)\n",
    "#     print(trg_mask.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):#, bad_idxs):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    i = 0\n",
    "    collection_of_losses = []\n",
    "    print(len(data_iter))\n",
    "    for _, batch in enumerate(data_iter):\n",
    "        #out, w2 = model.forward(batch.src, batch.trg, \n",
    "        #                    batch.src_mask, batch.trg_mask)\n",
    "        #print('ITERATION: ', _)\n",
    "        batch = batch.cuda()\n",
    "        batch = batch.long()\n",
    "        batch = batch.squeeze(0)\n",
    "        PAD = 0\n",
    "        trg = batch[:,:-1]\n",
    "        trg_y = batch[:, 1:]\n",
    "        src_mask = (batch != PAD).unsqueeze(-2)\n",
    "        trg_mask = (trg != PAD).unsqueeze(-2)\n",
    "        trg_mask = trg_mask & Variable(subsequent_mask(trg.size(-1)).type_as(trg_mask.data))\n",
    "        ntokens = (trg_y != PAD).data.sum()\n",
    "        \n",
    "        out, w2 = model.forward(batch, trg, src_mask, trg_mask)\n",
    "        #print(\"HEREEE\")\n",
    "        loss = loss_compute(out, trg_y, ntokens.item())\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += ntokens\n",
    "        tokens += ntokens\n",
    "        if i % 200 == 1:\n",
    "            collection_of_losses.append(torch.div(loss,ntokens).cpu())\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, torch.div(loss,ntokens), tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        i += 1\n",
    "    return (total_loss / total_tokens), collection_of_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trevi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "c:\\Users\\Trevi\\Desktop\\DRAGN\\MusGen\\src\\libs\\music_transformer.py:269: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "1177\n",
      "Epoch Step: 1 Loss: 6.319283 Tokens per Sec: 4206.986816\n",
      "Epoch Step: 201 Loss: 2.556851 Tokens per Sec: 80176.312500\n",
      "Epoch Step: 401 Loss: 3.002028 Tokens per Sec: 81426.406250\n",
      "Epoch Step: 601 Loss: 2.258176 Tokens per Sec: 79199.179688\n",
      "Epoch Step: 801 Loss: 2.394696 Tokens per Sec: 79630.898438\n",
      "Epoch Step: 1001 Loss: 1.857241 Tokens per Sec: 77323.351562\n",
      "epoch:  1\n",
      "1177\n",
      "Epoch Step: 1 Loss: 2.236180 Tokens per Sec: 78403.367188\n",
      "Epoch Step: 201 Loss: 2.260035 Tokens per Sec: 77784.351562\n",
      "Epoch Step: 401 Loss: 1.840687 Tokens per Sec: 76605.226562\n",
      "Epoch Step: 601 Loss: 1.965867 Tokens per Sec: 79206.203125\n",
      "Epoch Step: 801 Loss: 1.476752 Tokens per Sec: 79386.406250\n",
      "Epoch Step: 1001 Loss: 1.594057 Tokens per Sec: 78682.539062\n",
      "epoch:  2\n",
      "1177\n",
      "Epoch Step: 1 Loss: 1.996208 Tokens per Sec: 94860.679688\n",
      "Epoch Step: 201 Loss: 0.968014 Tokens per Sec: 79606.078125\n",
      "Epoch Step: 401 Loss: 1.307618 Tokens per Sec: 76144.617188\n",
      "Epoch Step: 601 Loss: 1.868902 Tokens per Sec: 77760.875000\n",
      "Epoch Step: 801 Loss: 0.844356 Tokens per Sec: 81086.195312\n",
      "Epoch Step: 1001 Loss: 2.119849 Tokens per Sec: 80698.750000\n",
      "epoch:  3\n",
      "1177\n",
      "Epoch Step: 1 Loss: 1.431080 Tokens per Sec: 70102.085938\n",
      "Epoch Step: 201 Loss: 0.962158 Tokens per Sec: 77650.085938\n",
      "Epoch Step: 401 Loss: 1.332156 Tokens per Sec: 76384.781250\n",
      "Epoch Step: 601 Loss: 1.358791 Tokens per Sec: 77903.476562\n",
      "Epoch Step: 801 Loss: 1.582440 Tokens per Sec: 78540.398438\n",
      "Epoch Step: 1001 Loss: 1.384935 Tokens per Sec: 78762.523438\n",
      "epoch:  4\n",
      "1177\n",
      "Epoch Step: 1 Loss: 1.524892 Tokens per Sec: 67303.953125\n",
      "Epoch Step: 201 Loss: 1.469362 Tokens per Sec: 74244.570312\n",
      "Epoch Step: 401 Loss: 1.499108 Tokens per Sec: 73371.687500\n",
      "Epoch Step: 601 Loss: 1.507290 Tokens per Sec: 72485.851562\n",
      "Epoch Step: 801 Loss: 0.764705 Tokens per Sec: 72942.875000\n",
      "Epoch Step: 1001 Loss: 1.917166 Tokens per Sec: 79303.046875\n",
      "epoch:  5\n",
      "1177\n",
      "Epoch Step: 1 Loss: 1.366229 Tokens per Sec: 76443.640625\n",
      "Epoch Step: 201 Loss: 1.403032 Tokens per Sec: 77962.390625\n",
      "Epoch Step: 401 Loss: 1.496103 Tokens per Sec: 83125.867188\n",
      "Epoch Step: 601 Loss: 2.164591 Tokens per Sec: 76719.695312\n",
      "Epoch Step: 801 Loss: 1.235226 Tokens per Sec: 81077.695312\n",
      "Epoch Step: 1001 Loss: 1.632864 Tokens per Sec: 80186.820312\n",
      "epoch:  6\n",
      "1177\n",
      "Epoch Step: 1 Loss: 1.624078 Tokens per Sec: 97344.445312\n",
      "Epoch Step: 201 Loss: 1.436894 Tokens per Sec: 80001.195312\n",
      "Epoch Step: 401 Loss: 1.524523 Tokens per Sec: 79072.656250\n",
      "Epoch Step: 601 Loss: 1.415291 Tokens per Sec: 80760.085938\n",
      "Epoch Step: 801 Loss: 1.376027 Tokens per Sec: 81756.890625\n",
      "Epoch Step: 1001 Loss: 1.471162 Tokens per Sec: 80628.226562\n",
      "epoch:  7\n",
      "1177\n",
      "Epoch Step: 1 Loss: 1.354890 Tokens per Sec: 89893.828125\n",
      "Epoch Step: 201 Loss: 1.715599 Tokens per Sec: 78671.593750\n",
      "Epoch Step: 401 Loss: 0.803717 Tokens per Sec: 81991.585938\n",
      "Epoch Step: 601 Loss: 1.358970 Tokens per Sec: 82938.445312\n",
      "Epoch Step: 801 Loss: 1.288805 Tokens per Sec: 81590.875000\n",
      "Epoch Step: 1001 Loss: 1.074509 Tokens per Sec: 78610.539062\n",
      "epoch:  8\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.908137 Tokens per Sec: 86751.328125\n",
      "Epoch Step: 201 Loss: 1.285975 Tokens per Sec: 78210.914062\n",
      "Epoch Step: 401 Loss: 1.367129 Tokens per Sec: 77182.164062\n",
      "Epoch Step: 601 Loss: 0.928707 Tokens per Sec: 80313.437500\n",
      "Epoch Step: 801 Loss: 1.546627 Tokens per Sec: 82016.421875\n",
      "Epoch Step: 1001 Loss: 1.034376 Tokens per Sec: 78988.210938\n",
      "epoch:  9\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.889122 Tokens per Sec: 64326.402344\n",
      "Epoch Step: 201 Loss: 1.286339 Tokens per Sec: 81291.492188\n",
      "Epoch Step: 401 Loss: 1.074792 Tokens per Sec: 79548.257812\n",
      "Epoch Step: 601 Loss: 0.931046 Tokens per Sec: 79546.960938\n",
      "Epoch Step: 801 Loss: 1.214029 Tokens per Sec: 80110.804688\n",
      "Epoch Step: 1001 Loss: 1.073308 Tokens per Sec: 81040.539062\n",
      "epoch:  10\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.924296 Tokens per Sec: 85797.554688\n",
      "Epoch Step: 201 Loss: 0.589337 Tokens per Sec: 78890.140625\n",
      "Epoch Step: 401 Loss: 0.971241 Tokens per Sec: 80012.820312\n",
      "Epoch Step: 601 Loss: 0.879113 Tokens per Sec: 79908.320312\n",
      "Epoch Step: 801 Loss: 0.747947 Tokens per Sec: 81245.679688\n",
      "Epoch Step: 1001 Loss: 0.332522 Tokens per Sec: 80515.148438\n",
      "epoch:  11\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.892005 Tokens per Sec: 77462.593750\n",
      "Epoch Step: 201 Loss: 0.820073 Tokens per Sec: 80931.367188\n",
      "Epoch Step: 401 Loss: 0.881484 Tokens per Sec: 79989.406250\n",
      "Epoch Step: 601 Loss: 0.819575 Tokens per Sec: 79274.585938\n",
      "Epoch Step: 801 Loss: 0.757849 Tokens per Sec: 79759.734375\n",
      "Epoch Step: 1001 Loss: 0.432849 Tokens per Sec: 82510.335938\n",
      "epoch:  12\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.635473 Tokens per Sec: 71291.187500\n",
      "Epoch Step: 201 Loss: 0.759646 Tokens per Sec: 78000.750000\n",
      "Epoch Step: 401 Loss: 0.615254 Tokens per Sec: 79700.296875\n",
      "Epoch Step: 601 Loss: 0.532672 Tokens per Sec: 79051.148438\n",
      "Epoch Step: 801 Loss: 0.471208 Tokens per Sec: 79187.609375\n",
      "Epoch Step: 1001 Loss: 0.509621 Tokens per Sec: 80194.632812\n",
      "epoch:  13\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.602174 Tokens per Sec: 72355.351562\n",
      "Epoch Step: 201 Loss: 0.576315 Tokens per Sec: 81270.320312\n",
      "Epoch Step: 401 Loss: 0.362197 Tokens per Sec: 79397.554688\n",
      "Epoch Step: 601 Loss: 0.394793 Tokens per Sec: 78951.351562\n",
      "Epoch Step: 801 Loss: 0.479670 Tokens per Sec: 81530.750000\n",
      "Epoch Step: 1001 Loss: 0.407452 Tokens per Sec: 80278.929688\n",
      "epoch:  14\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.307927 Tokens per Sec: 90621.593750\n",
      "Epoch Step: 201 Loss: 0.404749 Tokens per Sec: 95140.593750\n",
      "Epoch Step: 401 Loss: 0.465932 Tokens per Sec: 81447.593750\n",
      "Epoch Step: 601 Loss: 0.469624 Tokens per Sec: 79525.507812\n",
      "Epoch Step: 801 Loss: 0.563533 Tokens per Sec: 80502.640625\n",
      "Epoch Step: 1001 Loss: 0.112556 Tokens per Sec: 79108.851562\n",
      "epoch:  15\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.332216 Tokens per Sec: 75292.796875\n",
      "Epoch Step: 201 Loss: 0.327844 Tokens per Sec: 73623.812500\n",
      "Epoch Step: 401 Loss: 0.281465 Tokens per Sec: 79612.703125\n",
      "Epoch Step: 601 Loss: 0.208470 Tokens per Sec: 75475.179688\n",
      "Epoch Step: 801 Loss: 0.546357 Tokens per Sec: 79567.750000\n",
      "Epoch Step: 1001 Loss: 0.192058 Tokens per Sec: 81050.492188\n",
      "epoch:  16\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.288061 Tokens per Sec: 82278.070312\n",
      "Epoch Step: 201 Loss: 0.253117 Tokens per Sec: 79218.382812\n",
      "Epoch Step: 401 Loss: 0.255556 Tokens per Sec: 80151.312500\n",
      "Epoch Step: 601 Loss: 0.169772 Tokens per Sec: 79786.835938\n",
      "Epoch Step: 801 Loss: 0.259333 Tokens per Sec: 79715.968750\n",
      "Epoch Step: 1001 Loss: 0.137747 Tokens per Sec: 80584.226562\n",
      "epoch:  17\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.228986 Tokens per Sec: 60517.714844\n",
      "Epoch Step: 201 Loss: 0.150240 Tokens per Sec: 79481.109375\n",
      "Epoch Step: 401 Loss: 0.289566 Tokens per Sec: 80548.445312\n",
      "Epoch Step: 601 Loss: 0.211459 Tokens per Sec: 77757.984375\n",
      "Epoch Step: 801 Loss: 0.244400 Tokens per Sec: 78679.078125\n",
      "Epoch Step: 1001 Loss: 0.273724 Tokens per Sec: 78288.390625\n",
      "epoch:  18\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.309165 Tokens per Sec: 91543.812500\n",
      "Epoch Step: 201 Loss: 0.245774 Tokens per Sec: 79161.718750\n",
      "Epoch Step: 401 Loss: 0.178884 Tokens per Sec: 78932.320312\n",
      "Epoch Step: 601 Loss: 0.122588 Tokens per Sec: 79648.140625\n",
      "Epoch Step: 801 Loss: 0.202757 Tokens per Sec: 79526.726562\n",
      "Epoch Step: 1001 Loss: 0.177996 Tokens per Sec: 79511.289062\n",
      "epoch:  19\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.079699 Tokens per Sec: 84354.406250\n",
      "Epoch Step: 201 Loss: 0.060267 Tokens per Sec: 78068.843750\n",
      "Epoch Step: 401 Loss: 0.120691 Tokens per Sec: 78147.773438\n",
      "Epoch Step: 601 Loss: 0.191082 Tokens per Sec: 82168.250000\n",
      "Epoch Step: 801 Loss: 0.167733 Tokens per Sec: 75424.164062\n",
      "Epoch Step: 1001 Loss: 0.128042 Tokens per Sec: 79814.109375\n",
      "epoch:  20\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.155429 Tokens per Sec: 72279.171875\n",
      "Epoch Step: 201 Loss: 0.142958 Tokens per Sec: 77448.242188\n",
      "Epoch Step: 401 Loss: 0.117588 Tokens per Sec: 78501.617188\n",
      "Epoch Step: 601 Loss: 0.104593 Tokens per Sec: 79640.335938\n",
      "Epoch Step: 801 Loss: 0.081528 Tokens per Sec: 79965.468750\n",
      "Epoch Step: 1001 Loss: 0.176060 Tokens per Sec: 78698.476562\n",
      "epoch:  21\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.067467 Tokens per Sec: 79933.421875\n",
      "Epoch Step: 201 Loss: 0.040127 Tokens per Sec: 74886.281250\n",
      "Epoch Step: 401 Loss: 0.106550 Tokens per Sec: 80706.328125\n",
      "Epoch Step: 601 Loss: 0.107793 Tokens per Sec: 77249.734375\n",
      "Epoch Step: 801 Loss: 0.122384 Tokens per Sec: 75391.085938\n",
      "Epoch Step: 1001 Loss: 0.097330 Tokens per Sec: 77440.179688\n",
      "epoch:  22\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.099145 Tokens per Sec: 54656.992188\n",
      "Epoch Step: 201 Loss: 0.095927 Tokens per Sec: 77995.398438\n",
      "Epoch Step: 401 Loss: 0.076724 Tokens per Sec: 74004.203125\n",
      "Epoch Step: 601 Loss: 0.056117 Tokens per Sec: 77738.695312\n",
      "Epoch Step: 801 Loss: 0.108071 Tokens per Sec: 78848.648438\n",
      "Epoch Step: 1001 Loss: 0.092382 Tokens per Sec: 78059.671875\n",
      "epoch:  23\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.077590 Tokens per Sec: 80730.273438\n",
      "Epoch Step: 201 Loss: 0.058390 Tokens per Sec: 78923.015625\n",
      "Epoch Step: 401 Loss: 0.027935 Tokens per Sec: 81101.867188\n",
      "Epoch Step: 601 Loss: 0.090517 Tokens per Sec: 77530.140625\n",
      "Epoch Step: 801 Loss: 0.050089 Tokens per Sec: 77017.914062\n",
      "Epoch Step: 1001 Loss: 0.068295 Tokens per Sec: 82079.968750\n",
      "epoch:  24\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.074532 Tokens per Sec: 91302.273438\n",
      "Epoch Step: 201 Loss: 0.057352 Tokens per Sec: 80593.601562\n",
      "Epoch Step: 401 Loss: 0.058387 Tokens per Sec: 79125.757812\n",
      "Epoch Step: 601 Loss: 0.080387 Tokens per Sec: 77365.890625\n",
      "Epoch Step: 801 Loss: 0.042654 Tokens per Sec: 81153.148438\n",
      "Epoch Step: 1001 Loss: 0.037149 Tokens per Sec: 77427.585938\n",
      "epoch:  25\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.056452 Tokens per Sec: 91220.812500\n",
      "Epoch Step: 201 Loss: 0.052952 Tokens per Sec: 78648.523438\n",
      "Epoch Step: 401 Loss: 0.026980 Tokens per Sec: 80307.968750\n",
      "Epoch Step: 601 Loss: 0.041080 Tokens per Sec: 71328.234375\n",
      "Epoch Step: 801 Loss: 0.060191 Tokens per Sec: 75141.898438\n",
      "Epoch Step: 1001 Loss: 0.032763 Tokens per Sec: 75137.789062\n",
      "epoch:  26\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.046706 Tokens per Sec: 60198.898438\n",
      "Epoch Step: 201 Loss: 0.051836 Tokens per Sec: 74907.250000\n",
      "Epoch Step: 401 Loss: 0.049818 Tokens per Sec: 75512.273438\n",
      "Epoch Step: 601 Loss: 0.023147 Tokens per Sec: 74891.234375\n",
      "Epoch Step: 801 Loss: 0.025617 Tokens per Sec: 73140.351562\n",
      "Epoch Step: 1001 Loss: 0.019319 Tokens per Sec: 74475.015625\n",
      "epoch:  27\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.022170 Tokens per Sec: 91838.773438\n",
      "Epoch Step: 201 Loss: 0.030535 Tokens per Sec: 72855.796875\n",
      "Epoch Step: 401 Loss: 0.059780 Tokens per Sec: 75776.695312\n",
      "Epoch Step: 601 Loss: 0.017638 Tokens per Sec: 73925.203125\n",
      "Epoch Step: 801 Loss: 0.027445 Tokens per Sec: 75298.273438\n",
      "Epoch Step: 1001 Loss: 0.038813 Tokens per Sec: 77965.054688\n",
      "epoch:  28\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.027593 Tokens per Sec: 91376.351562\n",
      "Epoch Step: 201 Loss: 0.019062 Tokens per Sec: 78883.289062\n",
      "Epoch Step: 401 Loss: 0.030511 Tokens per Sec: 75933.750000\n",
      "Epoch Step: 601 Loss: 0.032168 Tokens per Sec: 75494.156250\n",
      "Epoch Step: 801 Loss: 0.020933 Tokens per Sec: 73718.515625\n",
      "Epoch Step: 1001 Loss: 0.036631 Tokens per Sec: 75591.546875\n",
      "epoch:  29\n",
      "1177\n",
      "Epoch Step: 1 Loss: 0.039165 Tokens per Sec: 85124.156250\n",
      "Epoch Step: 201 Loss: 0.029901 Tokens per Sec: 64500.898438\n",
      "Epoch Step: 401 Loss: 0.012996 Tokens per Sec: 64728.445312\n",
      "Epoch Step: 601 Loss: 0.022489 Tokens per Sec: 68641.656250\n",
      "Epoch Step: 801 Loss: 0.027602 Tokens per Sec: 68488.703125\n",
      "Epoch Step: 1001 Loss: 0.025051 Tokens per Sec: 76571.640625\n"
     ]
    }
   ],
   "source": [
    "# My version of Greedy Decoding\n",
    "def test_my_model():\n",
    "\n",
    "  #---------- TRAIN -----------\n",
    "  #V = len(all_data.vocab) # 312\n",
    "  V = 312\n",
    "  criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "  model = make_model(V, V, N=2, d_model=128)\n",
    "  model.cuda() # uncomment for GPU\n",
    "  model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0.6, betas=(0.9, 0.98), eps=1e-9))\n",
    "  #the_train_data = data_gen(all_data.train_ds)\n",
    "  the_train_data = dataloader\n",
    "  collection_of_losses = np.array([])\n",
    "  for epoch in range(30):\n",
    "    print(\"epoch: \", epoch)\n",
    "    model.train()\n",
    "    totalLoss_div_totalTokens, collection_of_losses_inp = run_epoch(the_train_data, model, SimpleLossCompute(model.generator, criterion, model_opt))#, bad_idxs)\n",
    "    #model.eval()\n",
    "    #print(run_epoch(the_train_data, model, SimpleLossCompute(model.generator, criterion, None), bad_idxs))\n",
    "    collection_of_losses = np.concatenate([collection_of_losses, collection_of_losses_inp])\n",
    "  return model, collection_of_losses\n",
    "\n",
    "model, losses = test_my_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to a .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../models/new_dataset_epochs_30_LM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Model from a previous state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = torch.load(\"./epochs_30_LM.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95fee283cad2380c3bbb086be18af1e1d950b8d84e571fde4cd1d4f314b30685"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
